Date,SPARQL endpoint,RDF dump,License Machine-Redeable,License Human-Redeable,KG name,Description,Url,Reliable,Trust value,Degree of connection,Clustring coefficient,Centrality,Number of sameAs chains,Number of triples,KGid,Min latency,25th percentile latency,Median latency,75th percentile latency,Max latency,Min TP,25th percentile TP,Median TP,75th percentile TP,Max TP,Requires auth,Use HTTPS,Serialization formats,Languages,Link SPARQL endpoint,Link for download the dataset,Number of void label,Number of whitespace label,Number of malformed datatype,Labels,Disjoint class,Undefined class,Undefined property,Deprecated class/property,Ontology hijacking,Misplaced property,Misplaced class,PageRank,Vocabularies,Authors,Publishers,Contributors,Sources,Signed,Dataset update frequency,Creation date,Modification date,Num. triples updated,Time since last modification,Extensional conciseness,Intensional conciseness,Interlinking completeness,Number of triples linked,Number of entities,Number of property,Min length URIs (subject),25th percentile length URIs (subject),Median length URIs (subject),75th percentile length URIs (subject),Max length URIs (subject),Min length URIs (predicate),25th percentile length URIs (predicate),Median length URIs (predicate),75th percentile length URIs (predicate),Max length URIs (predicate),Min length URIs (object),25th percentile length URIs (object),Median length URIs (object),75th percentile length URIs (object),Max length URIs (object),New vocabularies defined,New terms defined,Number of label,Uri regex,Presence of example,Number of blank nodes,RDF structures,HistoricalUp,Score,Normalized score,FP,IFP,Limited,License MR,Inactive links
2022-07-18,-1,0,Creative Commons Attribution - cc-by -,-,Wikilinks RDF/NIF,The [Wikilinks corpus](http://www.iesl.cs.umass.edu/data/wiki-links) is a coreference resolution corpus of very large scale. It contains over 40 million mentions of over 3 million entities. Mentions are manually labeled links to respective Wikipedia pages in natural language text. They are obtained via a web crawl and aggregated together with the pages' source in the extended corpus of over 180GB in size.We took the corpus and converted it into the [NLP Interchange Format (NIF)](http://nlp2rdf.org/)- publishing it here in Linked Open Data- RDF dumps and an accompanying CSV. Every webpage in the corpus was parsed. The text of the html element surrounding the individual Wikipedia links was extracted and concatenated together- if there was more than one link on the page. The position of the links in these texts was located and annotated via string offsets. The position of the html elements containing the links was annotated with Xpath expressions. For every link to Wikipedia- the respective [DBpedia](http://dbpedia.org) page was included as a link. The [DBpedia ontology classes](http://wiki.dbpedia.org/Ontology) of the linked resource were added as well. If a mapping exists- [NERD core classes](http://nerd.eurecom.fr/) were added- too.The data is available in the Apache file system under [http://wiki-link.nlp2rdf.org/data/](http://wiki-link.nlp2rdf.org/data/). However- for ease of use- it is also available in a number of [gzipped Dumpfiles](http://wiki-link.nlp2rdf.org/dumps/). Additionally- there is a [gzipped CSV file](http://wiki-link.nlp2rdf.org/wikilink_ne.csv.tar.gz) containing the core of the data. ,Absent,False,0.5,2,0,0.001,-,533016300,wikilinksrdfnif,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,-,-,endpoint absent,endpoint absent,nan,Not provided,-,-,-,endpoint absent,-,-,-,-,endpoint absent,-,-,0.005377710379181024,endpoint absent,Name: Martin Brümmer; Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,Web:Absent Name:Martin Brümmer Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,-,-,insufficient data,insufficient data,endpoint absent,endpoint absent,0.06,31542468,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,True,endpoint absent,endpoint absent,insufficient data,0.378,44.74,-,-,endpoint absent,endpoint absent,True
2022-07-31,-1,0,Creative Commons Attribution - cc-by -,-,Wikilinks RDF/NIF,The [Wikilinks corpus](http://www.iesl.cs.umass.edu/data/wiki-links) is a coreference resolution corpus of very large scale. It contains over 40 million mentions of over 3 million entities. Mentions are manually labeled links to respective Wikipedia pages in natural language text. They are obtained via a web crawl and aggregated together with the pages' source in the extended corpus of over 180GB in size.We took the corpus and converted it into the [NLP Interchange Format (NIF)](http://nlp2rdf.org/)- publishing it here in Linked Open Data- RDF dumps and an accompanying CSV. Every webpage in the corpus was parsed. The text of the html element surrounding the individual Wikipedia links was extracted and concatenated together- if there was more than one link on the page. The position of the links in these texts was located and annotated via string offsets. The position of the html elements containing the links was annotated with Xpath expressions. For every link to Wikipedia- the respective [DBpedia](http://dbpedia.org) page was included as a link. The [DBpedia ontology classes](http://wiki.dbpedia.org/Ontology) of the linked resource were added as well. If a mapping exists- [NERD core classes](http://nerd.eurecom.fr/) were added- too.The data is available in the Apache file system under [http://wiki-link.nlp2rdf.org/data/](http://wiki-link.nlp2rdf.org/data/). However- for ease of use- it is also available in a number of [gzipped Dumpfiles](http://wiki-link.nlp2rdf.org/dumps/). Additionally- there is a [gzipped CSV file](http://wiki-link.nlp2rdf.org/wikilink_ne.csv.tar.gz) containing the core of the data. ,Absent,False,0.5,2,0,0.001,-,533016300,wikilinksrdfnif,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,-,-,endpoint absent,endpoint absent,nan,Not provided,-,-,-,endpoint absent,-,-,-,-,endpoint absent,-,-,0.005377710379181024,endpoint absent,Name: Martin Brümmer; Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,Web:Absent Name:Martin Brümmer Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,-,-,insufficient data,insufficient data,endpoint absent,endpoint absent,0.06,31542468,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,True,endpoint absent,endpoint absent,insufficient data,0.378,0.1,-,-,endpoint absent,endpoint absent,True
2022-08-07,-1,0,Creative Commons Attribution - cc-by -,-,Wikilinks RDF/NIF,The [Wikilinks corpus](http://www.iesl.cs.umass.edu/data/wiki-links) is a coreference resolution corpus of very large scale. It contains over 40 million mentions of over 3 million entities. Mentions are manually labeled links to respective Wikipedia pages in natural language text. They are obtained via a web crawl and aggregated together with the pages' source in the extended corpus of over 180GB in size.We took the corpus and converted it into the [NLP Interchange Format (NIF)](http://nlp2rdf.org/)- publishing it here in Linked Open Data- RDF dumps and an accompanying CSV. Every webpage in the corpus was parsed. The text of the html element surrounding the individual Wikipedia links was extracted and concatenated together- if there was more than one link on the page. The position of the links in these texts was located and annotated via string offsets. The position of the html elements containing the links was annotated with Xpath expressions. For every link to Wikipedia- the respective [DBpedia](http://dbpedia.org) page was included as a link. The [DBpedia ontology classes](http://wiki.dbpedia.org/Ontology) of the linked resource were added as well. If a mapping exists- [NERD core classes](http://nerd.eurecom.fr/) were added- too.The data is available in the Apache file system under [http://wiki-link.nlp2rdf.org/data/](http://wiki-link.nlp2rdf.org/data/). However- for ease of use- it is also available in a number of [gzipped Dumpfiles](http://wiki-link.nlp2rdf.org/dumps/). Additionally- there is a [gzipped CSV file](http://wiki-link.nlp2rdf.org/wikilink_ne.csv.tar.gz) containing the core of the data. ,Absent,absent,0.5,2,0,0.001,-,533016300,wikilinksrdfnif,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,-,-,endpoint absent,endpoint absent,nan,Not provided,-,-,-,endpoint absent,-,-,-,-,endpoint absent,-,-,0.005377710379181024,endpoint absent,Name: Martin Brümmer; Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,Web:Absent Name:Martin Brümmer Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,-,-,insufficient data,insufficient data,endpoint absent,endpoint absent,0.06,31542468,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,True,endpoint absent,endpoint absent,insufficient data,0.378,0.1,-,-,endpoint absent,endpoint absent,True
2022-08-21,-1,0,Creative Commons Attribution - cc-by -,-,Wikilinks RDF/NIF,The [Wikilinks corpus](http://www.iesl.cs.umass.edu/data/wiki-links) is a coreference resolution corpus of very large scale. It contains over 40 million mentions of over 3 million entities. Mentions are manually labeled links to respective Wikipedia pages in natural language text. They are obtained via a web crawl and aggregated together with the pages' source in the extended corpus of over 180GB in size.We took the corpus and converted it into the [NLP Interchange Format (NIF)](http://nlp2rdf.org/)- publishing it here in Linked Open Data- RDF dumps and an accompanying CSV. Every webpage in the corpus was parsed. The text of the html element surrounding the individual Wikipedia links was extracted and concatenated together- if there was more than one link on the page. The position of the links in these texts was located and annotated via string offsets. The position of the html elements containing the links was annotated with Xpath expressions. For every link to Wikipedia- the respective [DBpedia](http://dbpedia.org) page was included as a link. The [DBpedia ontology classes](http://wiki.dbpedia.org/Ontology) of the linked resource were added as well. If a mapping exists- [NERD core classes](http://nerd.eurecom.fr/) were added- too.The data is available in the Apache file system under [http://wiki-link.nlp2rdf.org/data/](http://wiki-link.nlp2rdf.org/data/). However- for ease of use- it is also available in a number of [gzipped Dumpfiles](http://wiki-link.nlp2rdf.org/dumps/). Additionally- there is a [gzipped CSV file](http://wiki-link.nlp2rdf.org/wikilink_ne.csv.tar.gz) containing the core of the data. ,Absent,False,0.5,2,0,0.001,-,533016300,wikilinksrdfnif,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,-,-,endpoint absent,endpoint absent,nan,Not provided,-,-,-,endpoint absent,-,-,-,-,endpoint absent,-,-,0.005377710379181024,endpoint absent,Name: Martin Brümmer; Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,Web:Absent Name:Martin Brümmer Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,-,-,insufficient data,insufficient data,endpoint absent,endpoint absent,0.06,31542468,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,True,endpoint absent,endpoint absent,insufficient data,0.378,16.62,-,-,endpoint absent,endpoint absent,True
2022-08-28,-1,0,Creative Commons Attribution - cc-by -,-,Wikilinks RDF/NIF,The [Wikilinks corpus](http://www.iesl.cs.umass.edu/data/wiki-links) is a coreference resolution corpus of very large scale. It contains over 40 million mentions of over 3 million entities. Mentions are manually labeled links to respective Wikipedia pages in natural language text. They are obtained via a web crawl and aggregated together with the pages' source in the extended corpus of over 180GB in size.We took the corpus and converted it into the [NLP Interchange Format (NIF)](http://nlp2rdf.org/)- publishing it here in Linked Open Data- RDF dumps and an accompanying CSV. Every webpage in the corpus was parsed. The text of the html element surrounding the individual Wikipedia links was extracted and concatenated together- if there was more than one link on the page. The position of the links in these texts was located and annotated via string offsets. The position of the html elements containing the links was annotated with Xpath expressions. For every link to Wikipedia- the respective [DBpedia](http://dbpedia.org) page was included as a link. The [DBpedia ontology classes](http://wiki.dbpedia.org/Ontology) of the linked resource were added as well. If a mapping exists- [NERD core classes](http://nerd.eurecom.fr/) were added- too.The data is available in the Apache file system under [http://wiki-link.nlp2rdf.org/data/](http://wiki-link.nlp2rdf.org/data/). However- for ease of use- it is also available in a number of [gzipped Dumpfiles](http://wiki-link.nlp2rdf.org/dumps/). Additionally- there is a [gzipped CSV file](http://wiki-link.nlp2rdf.org/wikilink_ne.csv.tar.gz) containing the core of the data. ,Absent,False,0.5,2,0,0.001,-,533016300,wikilinksrdfnif,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,-,-,endpoint absent,endpoint absent,nan,Not provided,-,-,-,endpoint absent,-,-,-,-,endpoint absent,-,-,0.005377710379181024,endpoint absent,Name: Martin Brümmer; Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,Web:Absent Name:Martin Brümmer Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,-,-,insufficient data,insufficient data,endpoint absent,endpoint absent,0.06,31542468,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,True,endpoint absent,endpoint absent,insufficient data,0.378,16.62,-,-,endpoint absent,endpoint absent,True
2022-09-04,-1,-1,Creative Commons Attribution - cc-by -,-,Wikilinks RDF/NIF,The [Wikilinks corpus](http://www.iesl.cs.umass.edu/data/wiki-links) is a coreference resolution corpus of very large scale. It contains over 40 million mentions of over 3 million entities. Mentions are manually labeled links to respective Wikipedia pages in natural language text. They are obtained via a web crawl and aggregated together with the pages' source in the extended corpus of over 180GB in size.We took the corpus and converted it into the [NLP Interchange Format (NIF)](http://nlp2rdf.org/)- publishing it here in Linked Open Data- RDF dumps and an accompanying CSV. Every webpage in the corpus was parsed. The text of the html element surrounding the individual Wikipedia links was extracted and concatenated together- if there was more than one link on the page. The position of the links in these texts was located and annotated via string offsets. The position of the html elements containing the links was annotated with Xpath expressions. For every link to Wikipedia- the respective [DBpedia](http://dbpedia.org) page was included as a link. The [DBpedia ontology classes](http://wiki.dbpedia.org/Ontology) of the linked resource were added as well. If a mapping exists- [NERD core classes](http://nerd.eurecom.fr/) were added- too.The data is available in the Apache file system under [http://wiki-link.nlp2rdf.org/data/](http://wiki-link.nlp2rdf.org/data/). However- for ease of use- it is also available in a number of [gzipped Dumpfiles](http://wiki-link.nlp2rdf.org/dumps/). Additionally- there is a [gzipped CSV file](http://wiki-link.nlp2rdf.org/wikilink_ne.csv.tar.gz) containing the core of the data. ,Absent,False,0.5,2,0,0.001,-,533016300,wikilinksrdfnif,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,-,-,endpoint absent,endpoint absent,nan,Not provided,-,-,-,endpoint absent,-,-,-,-,endpoint absent,-,-,0.005377710379181024,endpoint absent,Name: Martin Brümmer; Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,Web:Absent Name:Martin Brümmer Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,-,-,insufficient data,insufficient data,endpoint absent,endpoint absent,0.06,31542468,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,True,endpoint absent,endpoint absent,insufficient data,0.378,0.12,-,-,endpoint absent,endpoint absent,True
2022-09-11,-1,-1,Creative Commons Attribution - cc-by -,-,Wikilinks RDF/NIF,The [Wikilinks corpus](http://www.iesl.cs.umass.edu/data/wiki-links) is a coreference resolution corpus of very large scale. It contains over 40 million mentions of over 3 million entities. Mentions are manually labeled links to respective Wikipedia pages in natural language text. They are obtained via a web crawl and aggregated together with the pages' source in the extended corpus of over 180GB in size.We took the corpus and converted it into the [NLP Interchange Format (NIF)](http://nlp2rdf.org/)- publishing it here in Linked Open Data- RDF dumps and an accompanying CSV. Every webpage in the corpus was parsed. The text of the html element surrounding the individual Wikipedia links was extracted and concatenated together- if there was more than one link on the page. The position of the links in these texts was located and annotated via string offsets. The position of the html elements containing the links was annotated with Xpath expressions. For every link to Wikipedia- the respective [DBpedia](http://dbpedia.org) page was included as a link. The [DBpedia ontology classes](http://wiki.dbpedia.org/Ontology) of the linked resource were added as well. If a mapping exists- [NERD core classes](http://nerd.eurecom.fr/) were added- too.The data is available in the Apache file system under [http://wiki-link.nlp2rdf.org/data/](http://wiki-link.nlp2rdf.org/data/). However- for ease of use- it is also available in a number of [gzipped Dumpfiles](http://wiki-link.nlp2rdf.org/dumps/). Additionally- there is a [gzipped CSV file](http://wiki-link.nlp2rdf.org/wikilink_ne.csv.tar.gz) containing the core of the data. ,Absent,False,0.5,2,0,0.001,-,533016300,wikilinksrdfnif,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,-,-,endpoint absent,endpoint absent,nan,Not provided,-,-,-,endpoint absent,-,-,-,-,endpoint absent,-,-,0.005377710379181024,endpoint absent,Name: Martin Brümmer; Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,Web:Absent Name:Martin Brümmer Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,-,-,insufficient data,insufficient data,endpoint absent,endpoint absent,0.06,31542468,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,True,endpoint absent,endpoint absent,insufficient data,0.378,0.12,-,-,endpoint absent,endpoint absent,True
2022-09-18,-1,-1,Creative Commons Attribution - cc-by -,-,Wikilinks RDF/NIF,The [Wikilinks corpus](http://www.iesl.cs.umass.edu/data/wiki-links) is a coreference resolution corpus of very large scale. It contains over 40 million mentions of over 3 million entities. Mentions are manually labeled links to respective Wikipedia pages in natural language text. They are obtained via a web crawl and aggregated together with the pages' source in the extended corpus of over 180GB in size.We took the corpus and converted it into the [NLP Interchange Format (NIF)](http://nlp2rdf.org/)- publishing it here in Linked Open Data- RDF dumps and an accompanying CSV. Every webpage in the corpus was parsed. The text of the html element surrounding the individual Wikipedia links was extracted and concatenated together- if there was more than one link on the page. The position of the links in these texts was located and annotated via string offsets. The position of the html elements containing the links was annotated with Xpath expressions. For every link to Wikipedia- the respective [DBpedia](http://dbpedia.org) page was included as a link. The [DBpedia ontology classes](http://wiki.dbpedia.org/Ontology) of the linked resource were added as well. If a mapping exists- [NERD core classes](http://nerd.eurecom.fr/) were added- too.The data is available in the Apache file system under [http://wiki-link.nlp2rdf.org/data/](http://wiki-link.nlp2rdf.org/data/). However- for ease of use- it is also available in a number of [gzipped Dumpfiles](http://wiki-link.nlp2rdf.org/dumps/). Additionally- there is a [gzipped CSV file](http://wiki-link.nlp2rdf.org/wikilink_ne.csv.tar.gz) containing the core of the data. ,Absent,False,0.5,2,0,0.001,-,533016300,wikilinksrdfnif,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,-,-,endpoint absent,endpoint absent,nan,Not provided,-,-,-,endpoint absent,-,-,-,-,endpoint absent,-,-,0.005377710379181024,endpoint absent,Name: Martin Brümmer; Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,Web:Absent Name:Martin Brümmer Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,-,-,insufficient data,insufficient data,endpoint absent,endpoint absent,0.06,31542468,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,True,endpoint absent,endpoint absent,insufficient data,0.378,14.26,-,-,endpoint absent,endpoint absent,True
2022-10-02,-1,-1,Creative Commons Attribution - cc-by -,-,Wikilinks RDF/NIF,The [Wikilinks corpus](http://www.iesl.cs.umass.edu/data/wiki-links) is a coreference resolution corpus of very large scale. It contains over 40 million mentions of over 3 million entities. Mentions are manually labeled links to respective Wikipedia pages in natural language text. They are obtained via a web crawl and aggregated together with the pages' source in the extended corpus of over 180GB in size.We took the corpus and converted it into the [NLP Interchange Format (NIF)](http://nlp2rdf.org/)- publishing it here in Linked Open Data- RDF dumps and an accompanying CSV. Every webpage in the corpus was parsed. The text of the html element surrounding the individual Wikipedia links was extracted and concatenated together- if there was more than one link on the page. The position of the links in these texts was located and annotated via string offsets. The position of the html elements containing the links was annotated with Xpath expressions. For every link to Wikipedia- the respective [DBpedia](http://dbpedia.org) page was included as a link. The [DBpedia ontology classes](http://wiki.dbpedia.org/Ontology) of the linked resource were added as well. If a mapping exists- [NERD core classes](http://nerd.eurecom.fr/) were added- too.The data is available in the Apache file system under [http://wiki-link.nlp2rdf.org/data/](http://wiki-link.nlp2rdf.org/data/). However- for ease of use- it is also available in a number of [gzipped Dumpfiles](http://wiki-link.nlp2rdf.org/dumps/). Additionally- there is a [gzipped CSV file](http://wiki-link.nlp2rdf.org/wikilink_ne.csv.tar.gz) containing the core of the data. ,Absent,False,0.5,2,0,0.001,-,533016300,wikilinksrdfnif,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,-,-,endpoint absent,endpoint absent,nan,Not provided,-,-,-,endpoint absent,-,-,-,-,endpoint absent,-,-,0.005377710379181024,endpoint absent,Name: Martin Brümmer; Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,Web:Absent Name:Martin Brümmer Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,-,-,insufficient data,insufficient data,endpoint absent,endpoint absent,0.06,31542468,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,True,endpoint absent,endpoint absent,insufficient data,0.378,14.26,-,-,endpoint absent,endpoint absent,True
2022-10-09,-1,-1,Creative Commons Attribution - cc-by -,-,Wikilinks RDF/NIF,The [Wikilinks corpus](http://www.iesl.cs.umass.edu/data/wiki-links) is a coreference resolution corpus of very large scale. It contains over 40 million mentions of over 3 million entities. Mentions are manually labeled links to respective Wikipedia pages in natural language text. They are obtained via a web crawl and aggregated together with the pages' source in the extended corpus of over 180GB in size.We took the corpus and converted it into the [NLP Interchange Format (NIF)](http://nlp2rdf.org/)- publishing it here in Linked Open Data- RDF dumps and an accompanying CSV. Every webpage in the corpus was parsed. The text of the html element surrounding the individual Wikipedia links was extracted and concatenated together- if there was more than one link on the page. The position of the links in these texts was located and annotated via string offsets. The position of the html elements containing the links was annotated with Xpath expressions. For every link to Wikipedia- the respective [DBpedia](http://dbpedia.org) page was included as a link. The [DBpedia ontology classes](http://wiki.dbpedia.org/Ontology) of the linked resource were added as well. If a mapping exists- [NERD core classes](http://nerd.eurecom.fr/) were added- too.The data is available in the Apache file system under [http://wiki-link.nlp2rdf.org/data/](http://wiki-link.nlp2rdf.org/data/). However- for ease of use- it is also available in a number of [gzipped Dumpfiles](http://wiki-link.nlp2rdf.org/dumps/). Additionally- there is a [gzipped CSV file](http://wiki-link.nlp2rdf.org/wikilink_ne.csv.tar.gz) containing the core of the data. ,Absent,False,0.5,2,0,0.001,-,533016300,wikilinksrdfnif,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,-,-,endpoint absent,endpoint absent,nan,Not provided,-,-,-,endpoint absent,-,-,-,-,endpoint absent,-,-,0.005377710379181024,endpoint absent,Name: Martin Brümmer; Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,Web:Absent Name:Martin Brümmer Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,-,-,insufficient data,insufficient data,endpoint absent,endpoint absent,0.06,31542468,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,True,endpoint absent,endpoint absent,insufficient data,0.378,14.26,-,-,endpoint absent,endpoint absent,True
2022-10-16,-1,-1,Creative Commons Attribution - cc-by -,-,Wikilinks RDF/NIF,The [Wikilinks corpus](http://www.iesl.cs.umass.edu/data/wiki-links) is a coreference resolution corpus of very large scale. It contains over 40 million mentions of over 3 million entities. Mentions are manually labeled links to respective Wikipedia pages in natural language text. They are obtained via a web crawl and aggregated together with the pages' source in the extended corpus of over 180GB in size.We took the corpus and converted it into the [NLP Interchange Format (NIF)](http://nlp2rdf.org/)- publishing it here in Linked Open Data- RDF dumps and an accompanying CSV. Every webpage in the corpus was parsed. The text of the html element surrounding the individual Wikipedia links was extracted and concatenated together- if there was more than one link on the page. The position of the links in these texts was located and annotated via string offsets. The position of the html elements containing the links was annotated with Xpath expressions. For every link to Wikipedia- the respective [DBpedia](http://dbpedia.org) page was included as a link. The [DBpedia ontology classes](http://wiki.dbpedia.org/Ontology) of the linked resource were added as well. If a mapping exists- [NERD core classes](http://nerd.eurecom.fr/) were added- too.The data is available in the Apache file system under [http://wiki-link.nlp2rdf.org/data/](http://wiki-link.nlp2rdf.org/data/). However- for ease of use- it is also available in a number of [gzipped Dumpfiles](http://wiki-link.nlp2rdf.org/dumps/). Additionally- there is a [gzipped CSV file](http://wiki-link.nlp2rdf.org/wikilink_ne.csv.tar.gz) containing the core of the data. ,Absent,False,0.5,2,0,0.001,-,533016300,wikilinksrdfnif,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,-,-,endpoint absent,endpoint absent,nan,Not provided,-,-,-,endpoint absent,-,-,-,-,endpoint absent,-,-,0.005377710379181024,endpoint absent,Name: Martin Brümmer; Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,Web:Absent Name:Martin Brümmer Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,-,-,insufficient data,insufficient data,endpoint absent,endpoint absent,0.06,31542468,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,True,endpoint absent,endpoint absent,insufficient data,0.378,14.26,-,-,endpoint absent,endpoint absent,True
2022-10-23,-1,-1,Creative Commons Attribution - cc-by -,-,Wikilinks RDF/NIF,The [Wikilinks corpus](http://www.iesl.cs.umass.edu/data/wiki-links) is a coreference resolution corpus of very large scale. It contains over 40 million mentions of over 3 million entities. Mentions are manually labeled links to respective Wikipedia pages in natural language text. They are obtained via a web crawl and aggregated together with the pages' source in the extended corpus of over 180GB in size.We took the corpus and converted it into the [NLP Interchange Format (NIF)](http://nlp2rdf.org/)- publishing it here in Linked Open Data- RDF dumps and an accompanying CSV. Every webpage in the corpus was parsed. The text of the html element surrounding the individual Wikipedia links was extracted and concatenated together- if there was more than one link on the page. The position of the links in these texts was located and annotated via string offsets. The position of the html elements containing the links was annotated with Xpath expressions. For every link to Wikipedia- the respective [DBpedia](http://dbpedia.org) page was included as a link. The [DBpedia ontology classes](http://wiki.dbpedia.org/Ontology) of the linked resource were added as well. If a mapping exists- [NERD core classes](http://nerd.eurecom.fr/) were added- too.The data is available in the Apache file system under [http://wiki-link.nlp2rdf.org/data/](http://wiki-link.nlp2rdf.org/data/). However- for ease of use- it is also available in a number of [gzipped Dumpfiles](http://wiki-link.nlp2rdf.org/dumps/). Additionally- there is a [gzipped CSV file](http://wiki-link.nlp2rdf.org/wikilink_ne.csv.tar.gz) containing the core of the data. ,Absent,absent,0.5,2,0,0.001,-,533016300,wikilinksrdfnif,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,-,-,endpoint absent,endpoint absent,nan,Not provided,-,-,-,endpoint absent,-,-,-,-,endpoint absent,-,-,0.005377710379181024,endpoint absent,Name: Martin Brümmer; Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,Web:Absent Name:Martin Brümmer Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,-,-,insufficient data,insufficient data,endpoint absent,endpoint absent,0.06,31542468,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,True,endpoint absent,endpoint absent,insufficient data,0.378,14.26,-,-,endpoint absent,endpoint absent,True
2022-11-06,-1,-1,Creative Commons Attribution - cc-by -,-,Wikilinks RDF/NIF,The [Wikilinks corpus](http://www.iesl.cs.umass.edu/data/wiki-links) is a coreference resolution corpus of very large scale. It contains over 40 million mentions of over 3 million entities. Mentions are manually labeled links to respective Wikipedia pages in natural language text. They are obtained via a web crawl and aggregated together with the pages' source in the extended corpus of over 180GB in size.We took the corpus and converted it into the [NLP Interchange Format (NIF)](http://nlp2rdf.org/)- publishing it here in Linked Open Data- RDF dumps and an accompanying CSV. Every webpage in the corpus was parsed. The text of the html element surrounding the individual Wikipedia links was extracted and concatenated together- if there was more than one link on the page. The position of the links in these texts was located and annotated via string offsets. The position of the html elements containing the links was annotated with Xpath expressions. For every link to Wikipedia- the respective [DBpedia](http://dbpedia.org) page was included as a link. The [DBpedia ontology classes](http://wiki.dbpedia.org/Ontology) of the linked resource were added as well. If a mapping exists- [NERD core classes](http://nerd.eurecom.fr/) were added- too.The data is available in the Apache file system under [http://wiki-link.nlp2rdf.org/data/](http://wiki-link.nlp2rdf.org/data/). However- for ease of use- it is also available in a number of [gzipped Dumpfiles](http://wiki-link.nlp2rdf.org/dumps/). Additionally- there is a [gzipped CSV file](http://wiki-link.nlp2rdf.org/wikilink_ne.csv.tar.gz) containing the core of the data. ,Absent,False,0.5,2,0,0.001,-,533016300,wikilinksrdfnif,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,-,-,endpoint absent,endpoint absent,nan,Not provided,-,-,-,endpoint absent,-,-,-,-,endpoint absent,-,-,0.005377710379181024,endpoint absent,Name: Martin Brümmer; Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,Web:Absent Name:Martin Brümmer Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,-,-,insufficient data,insufficient data,endpoint absent,endpoint absent,0.06,31542468,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,True,endpoint absent,endpoint absent,insufficient data,0.378,14.26,-,-,endpoint absent,endpoint absent,True
2022-11-13,-1,-1,Creative Commons Attribution - cc-by -,-,Wikilinks RDF/NIF,The [Wikilinks corpus](http://www.iesl.cs.umass.edu/data/wiki-links) is a coreference resolution corpus of very large scale. It contains over 40 million mentions of over 3 million entities. Mentions are manually labeled links to respective Wikipedia pages in natural language text. They are obtained via a web crawl and aggregated together with the pages' source in the extended corpus of over 180GB in size.We took the corpus and converted it into the [NLP Interchange Format (NIF)](http://nlp2rdf.org/)- publishing it here in Linked Open Data- RDF dumps and an accompanying CSV. Every webpage in the corpus was parsed. The text of the html element surrounding the individual Wikipedia links was extracted and concatenated together- if there was more than one link on the page. The position of the links in these texts was located and annotated via string offsets. The position of the html elements containing the links was annotated with Xpath expressions. For every link to Wikipedia- the respective [DBpedia](http://dbpedia.org) page was included as a link. The [DBpedia ontology classes](http://wiki.dbpedia.org/Ontology) of the linked resource were added as well. If a mapping exists- [NERD core classes](http://nerd.eurecom.fr/) were added- too.The data is available in the Apache file system under [http://wiki-link.nlp2rdf.org/data/](http://wiki-link.nlp2rdf.org/data/). However- for ease of use- it is also available in a number of [gzipped Dumpfiles](http://wiki-link.nlp2rdf.org/dumps/). Additionally- there is a [gzipped CSV file](http://wiki-link.nlp2rdf.org/wikilink_ne.csv.tar.gz) containing the core of the data. ,Absent,False,0.5,2,0,0.001,-,533016300,wikilinksrdfnif,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,-,-,endpoint absent,endpoint absent,nan,Not provided,-,-,-,endpoint absent,-,-,-,-,endpoint absent,-,-,0.005377710379181024,endpoint absent,Name: Martin Brümmer; Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,Web:Absent Name:Martin Brümmer Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,-,-,insufficient data,insufficient data,endpoint absent,endpoint absent,0.06,31542468,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,True,endpoint absent,endpoint absent,insufficient data,0.378,14.26,-,-,endpoint absent,endpoint absent,True
2022-11-20,-1,-1,Creative Commons Attribution - cc-by -,-,Wikilinks RDF/NIF,The [Wikilinks corpus](http://www.iesl.cs.umass.edu/data/wiki-links) is a coreference resolution corpus of very large scale. It contains over 40 million mentions of over 3 million entities. Mentions are manually labeled links to respective Wikipedia pages in natural language text. They are obtained via a web crawl and aggregated together with the pages' source in the extended corpus of over 180GB in size.We took the corpus and converted it into the [NLP Interchange Format (NIF)](http://nlp2rdf.org/)- publishing it here in Linked Open Data- RDF dumps and an accompanying CSV. Every webpage in the corpus was parsed. The text of the html element surrounding the individual Wikipedia links was extracted and concatenated together- if there was more than one link on the page. The position of the links in these texts was located and annotated via string offsets. The position of the html elements containing the links was annotated with Xpath expressions. For every link to Wikipedia- the respective [DBpedia](http://dbpedia.org) page was included as a link. The [DBpedia ontology classes](http://wiki.dbpedia.org/Ontology) of the linked resource were added as well. If a mapping exists- [NERD core classes](http://nerd.eurecom.fr/) were added- too.The data is available in the Apache file system under [http://wiki-link.nlp2rdf.org/data/](http://wiki-link.nlp2rdf.org/data/). However- for ease of use- it is also available in a number of [gzipped Dumpfiles](http://wiki-link.nlp2rdf.org/dumps/). Additionally- there is a [gzipped CSV file](http://wiki-link.nlp2rdf.org/wikilink_ne.csv.tar.gz) containing the core of the data. ,Absent,absent,0.5,2,0,0.001,-,533016300,wikilinksrdfnif,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,-,-,endpoint absent,endpoint absent,nan,Not provided,-,-,-,endpoint absent,-,-,-,-,endpoint absent,-,-,0.005377710379181024,endpoint absent,Name: Martin Brümmer; Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,Web:Absent Name:Martin Brümmer Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,-,-,insufficient data,insufficient data,endpoint absent,endpoint absent,0.06,31542468,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,True,endpoint absent,endpoint absent,insufficient data,0.378,14.26,-,-,endpoint absent,endpoint absent,True
2022-11-27,-1,-1,Creative Commons Attribution - cc-by -,-,Wikilinks RDF/NIF,The [Wikilinks corpus](http://www.iesl.cs.umass.edu/data/wiki-links) is a coreference resolution corpus of very large scale. It contains over 40 million mentions of over 3 million entities. Mentions are manually labeled links to respective Wikipedia pages in natural language text. They are obtained via a web crawl and aggregated together with the pages' source in the extended corpus of over 180GB in size.We took the corpus and converted it into the [NLP Interchange Format (NIF)](http://nlp2rdf.org/)- publishing it here in Linked Open Data- RDF dumps and an accompanying CSV. Every webpage in the corpus was parsed. The text of the html element surrounding the individual Wikipedia links was extracted and concatenated together- if there was more than one link on the page. The position of the links in these texts was located and annotated via string offsets. The position of the html elements containing the links was annotated with Xpath expressions. For every link to Wikipedia- the respective [DBpedia](http://dbpedia.org) page was included as a link. The [DBpedia ontology classes](http://wiki.dbpedia.org/Ontology) of the linked resource were added as well. If a mapping exists- [NERD core classes](http://nerd.eurecom.fr/) were added- too.The data is available in the Apache file system under [http://wiki-link.nlp2rdf.org/data/](http://wiki-link.nlp2rdf.org/data/). However- for ease of use- it is also available in a number of [gzipped Dumpfiles](http://wiki-link.nlp2rdf.org/dumps/). Additionally- there is a [gzipped CSV file](http://wiki-link.nlp2rdf.org/wikilink_ne.csv.tar.gz) containing the core of the data. ,Absent,absent,0.5,2,0,0.001,-,533016300,wikilinksrdfnif,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,-,-,endpoint absent,endpoint absent,nan,Not provided,-,-,-,endpoint absent,-,-,-,-,endpoint absent,-,-,0.005377710379181024,endpoint absent,Name: Martin Brümmer; Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,Web:Absent Name:Martin Brümmer Email:bruemmer@informatik.uni-leipzig.de,endpoint absent,endpoint absent,-,-,insufficient data,insufficient data,endpoint absent,endpoint absent,0.06,31542468,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,endpoint absent,True,endpoint absent,endpoint absent,insufficient data,0.378,14.26,-,-,endpoint absent,endpoint absent,True
